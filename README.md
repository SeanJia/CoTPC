# CoTPC
Chain-of-Thought Predictive Control

### Tasks
Currently the code includes four tasks for state-based imitation learning: 
`PickCube-v0`, `StackCube-v0`, `PegInsertionSide-v0` and `TurnFaucet-v0`.

### Demonstration Data
The state-based demo trajectories used in the CoTPC paper are stored in this Googld Drive [folder](https://drive.google.com/drive/folders/1VdunXUlzqAvy-D8MniQ4anhV5LLBfNbJ).
Each folder has a `*.h5` file (for actual trajectories) and a `*.json` file (for metadata regarding the trajectories).
Each task has approximately 1000 trajectories and each comes with a different env variation (i.e., env seed).
These demos are generated by replaying the official demos provided by [ManiSkill2](https://github.com/haosulab/ManiSkill2#demonstrations) using `replay_trajectory.py` with `control_mode="pd_joint_delta_pos"`, `obs_mode="state"`, and several patches to the ManiSkill2 code (see `maniskill2_patches`).
Specifically, we add additional flags to the tasks so that the key states (the Chain-of-Thought) can be obtained with priviledged information from the simulator.
For the task `TurnFaucet-v0`, we use a subset of 10 faucet models for the demos (see `scripts/replay_turn_faucet_trajectories.sh`).
If you want to generate visual-based demos, please refer to the official ManiSkill2 [repo](https://github.com/haosulab/ManiSkill2#demonstrations).

### Data Loader
The data loader samples (contiguous) subarrays of demo trajectories by specifying the min and max sample lengths. 
In CoTPC, we simply use a fixed value for both min and max (e.g., set the context size as 60 for all tasks).
With a fixed random seed `seed` and `num_traj`, each time the data loader samples a fixed subset of all trajectories.
In all experiments from the paper, we use `seed=0` and `num_traj=500`.
For TurnFaucet, due to the variations of the different faucet models, the loader performs sampling such that the number of trajs
per faucet model is the same (hopefully this balanced data ease model training).
The key states (a.k.a the Chain-of-Thought) can be obtained with the function `get_key_states(...)` that accesses privileged info available during training.

### Evaluation
The patches to the environments in ManiSkill2 as described previously also provide additional evaluation metrics (intermediate success rate) for trajectories.
To evaluate a model using the same set of env seeds as the ones used in demos (or using a specific set of env seeds), please refer to this official [doc](https://github.com/haosulab/ManiSkill2/blob/main/docs/source/concepts/demonstrations.md). 
<!-- Since the current focus is to evaluate the training performance (i.e., evaluate the learned policy on the env variations used during BC), -->
<!-- I provide a similar template for loading the gym envs as for loading demos in `data_loader.py`. -->
<!-- Given the same `seed` and `num_traj` as in training, this should give you the same set of envs used for generating the demos. -->
<!-- I equip it with `vec_env.py` to boost up the evaluation process (it will still take several minutes to evaluate on 500 envs, FYI). -->
<!-- The metrics used here are `success` and flags for some other intermediate key states specific to each task. -->
<!-- If at any timestep a metric == True, I just report True for that metric for the trajectory. -->
<!-- We also set a maximum timesteps allowed for each of the four tasks (see details in `eval_starter.py`). -->

### Sampled Training Scripts
Please see `scripts/train.sh` as examples.